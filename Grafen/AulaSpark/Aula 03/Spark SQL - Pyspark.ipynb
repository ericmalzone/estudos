{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "from IPython.display import Image\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#importando as classes que possuem as funcoes do Spark que vamos utilizar nessa aula\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"titanic.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"TBtitanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|Name                                               |\n",
      "+---------------------------------------------------+\n",
      "|Braund, Mr. Owen Harris                            |\n",
      "|Cumings, Mrs. John Bradley (Florence Briggs Thayer)|\n",
      "|Heikkinen, Miss. Laina                             |\n",
      "|Futrelle, Mrs. Jacques Heath (Lily May Peel)       |\n",
      "|Allen, Mr. William Henry                           |\n",
      "+---------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select Name from TBtitanic\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from TBtitanic\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulando um Dataframe com Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+---------------------------------------------------+\n",
      "|NovaColunaName                                     |Name                                               |\n",
      "+---------------------------------------------------+---------------------------------------------------+\n",
      "|Braund, Mr. Owen Harris                            |Braund, Mr. Owen Harris                            |\n",
      "|Cumings, Mrs. John Bradley (Florence Briggs Thayer)|Cumings, Mrs. John Bradley (Florence Briggs Thayer)|\n",
      "|Heikkinen, Miss. Laina                             |Heikkinen, Miss. Laina                             |\n",
      "|Futrelle, Mrs. Jacques Heath (Lily May Peel)       |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |\n",
      "|Allen, Mr. William Henry                           |Allen, Mr. William Henry                           |\n",
      "+---------------------------------------------------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .selectExpr()\n",
    "# df.selectExpr(\"Name as NovaColunaName\", \"Name\").show(2, truncate=False)\n",
    "spark.sql(\"Select Name as NovaColunaName, Name from TBtitanic limit 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercicio:\n",
    "\n",
    "Em Spark SQL:\n",
    "\n",
    "1. Carregar o parquet tabelaEstado.parquet e criar nova coluna com a CAPITAL em maiusculo\n",
    "2. Carregar o parquet titanic.parquet e contar quantas pessoas sobreviveram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"tabelaEstado.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"TBestado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------+\n",
      "|  ESTADO|SIGLA|     CAPITAL|\n",
      "+--------+-----+------------+\n",
      "|    Acre|   AC|\"Rio Branco\"|\n",
      "| Alagoas|   AL|      Maceio|\n",
      "|   Amapa|   AP|      Macapa|\n",
      "|Amazonas|   AM|      Manaus|\n",
      "|   Bahia|   BA|    Salvador|\n",
      "+--------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from TBestado\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|CAPITAL_MAIUSCULA|\n",
      "+-----------------+\n",
      "|     \"RIO BRANCO\"|\n",
      "|           MACEIO|\n",
      "|           MACAPA|\n",
      "|           MANAUS|\n",
      "|         SALVADOR|\n",
      "|        FORTALEZA|\n",
      "|         BRASILIA|\n",
      "|          VITORIA|\n",
      "|          GOIANIA|\n",
      "|       \"SAO LUIS\"|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select UPPER(CAPITAL) as CAPITAL_MAIUSCULA from TBestado\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"titanic.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"TBtitanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Sobreviventes|\n",
      "+-------------+\n",
      "|          342|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select count(*) as Sobreviventes from TBtitanic where Survived = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+------------------+\n",
      "|Name                                               |Nemesis           |\n",
      "+---------------------------------------------------+------------------+\n",
      "|Braund, Mr. Owen Harris                            |Criando um literal|\n",
      "|Cumings, Mrs. John Bradley (Florence Briggs Thayer)|Criando um literal|\n",
      "|Heikkinen, Miss. Laina                             |Criando um literal|\n",
      "|Futrelle, Mrs. Jacques Heath (Lily May Peel)       |Criando um literal|\n",
      "|Allen, Mr. William Henry                           |Criando um literal|\n",
      "+---------------------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .lit()\n",
    "# df.select(col(\"Name\"),lit(\"Criando um literal\")).show(2,False)\n",
    "spark.sql(\"Select Name, 'Criando um literal' as Nemesis from TBtitanic limit 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Age |\n",
      "+----+\n",
      "|22.0|\n",
      "|38.0|\n",
      "|26.0|\n",
      "|35.0|\n",
      "|35.0|\n",
      "|null|\n",
      "|54.0|\n",
      "|2.0 |\n",
      "|27.0|\n",
      "|14.0|\n",
      "|4.0 |\n",
      "|58.0|\n",
      "|20.0|\n",
      "|39.0|\n",
      "|14.0|\n",
      "|55.0|\n",
      "|2.0 |\n",
      "|null|\n",
      "|31.0|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select Age from TBtitanic limit 20\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .withColumn()\n",
    "# df.withColumn(\"NovaColuna\", expr(\"CASE WHEN (age > 20) THEN 'Maior' ELSE 'Menor' END\"))\\\n",
    "#  .select(col(\"NovaColuna\"),col(\"Age\")).show(10,False)\n",
    "spark.sql(\"Select CASE WHEN (age > 20) THEN 'Maior' ELSE 'Menor' END as NovaColuna, Cast(Age as Int) \"\\\n",
    "          \"from TBtitanic limit 10\")\\\n",
    "            .filter(col(\"Age\") != 0).orderBy(col(\"Age\"))\\\n",
    "            .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercicio:\n",
    "\n",
    "Em Spark SQL:\n",
    "\n",
    "1. \n",
    "        Carregar o parquet titanic.parquet, filtrar as pessoas que são maiores de 18 anos,\n",
    "        Criar nova coluna com o seguinte texto \"Pessoas maiores de idade\",\n",
    "        Trazer somente 50 linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"titanic.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"TBtitanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------------+\n",
      "|Age |texto                     |\n",
      "+----+--------------------------+\n",
      "|22.0|Pessoas maiores de 18 anos|\n",
      "|38.0|Pessoas maiores de 18 anos|\n",
      "|26.0|Pessoas maiores de 18 anos|\n",
      "|35.0|Pessoas maiores de 18 anos|\n",
      "|35.0|Pessoas maiores de 18 anos|\n",
      "|54.0|Pessoas maiores de 18 anos|\n",
      "|27.0|Pessoas maiores de 18 anos|\n",
      "|58.0|Pessoas maiores de 18 anos|\n",
      "|20.0|Pessoas maiores de 18 anos|\n",
      "|39.0|Pessoas maiores de 18 anos|\n",
      "|55.0|Pessoas maiores de 18 anos|\n",
      "|31.0|Pessoas maiores de 18 anos|\n",
      "|35.0|Pessoas maiores de 18 anos|\n",
      "|34.0|Pessoas maiores de 18 anos|\n",
      "|28.0|Pessoas maiores de 18 anos|\n",
      "|38.0|Pessoas maiores de 18 anos|\n",
      "|19.0|Pessoas maiores de 18 anos|\n",
      "|40.0|Pessoas maiores de 18 anos|\n",
      "|66.0|Pessoas maiores de 18 anos|\n",
      "|28.0|Pessoas maiores de 18 anos|\n",
      "+----+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select Age, 'Pessoas maiores de 18 anos' as texto from TBtitanic where Age > 18 limit 50\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark SQL pode ser construído com ou sem o Apache Hive, o mecanismo SQL do Hadoop. O suporte ao Spark SQL com Hive nos permite acessar tabelas Hive e a linguagem de consulta Hive (HiveQL). É importante observar que incluir as bibliotecas Hive não requer uma instalação. Em geral, é melhor criar o Spark SQL com suporte ao Hive para acessar esses recursos. \n",
    "\n",
    "Se você baixar o Spark em formato binário, ele já deverá ser compilado com o suporte do Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo uso para buscar dados do HIVE na versão 1.6:\n",
    "#hc = HiveContext(sc)\n",
    "\n",
    "#hc.table(\"database.tabela\")\n",
    "#    .select(col(\"AAA\").alias(\"NOME_COLUNA\"))\n",
    "\n",
    "#2.x\n",
    "#spark.table(\"database.tabela\").select(col(\"AAA\").alias(\"NOME_COLUNA\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
